{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4I5rAbbWy3smOWDKmQIf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KutuDev/DataCentricAIComp/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHpFLzYN4MnD"
      },
      "source": [
        "[Announcement worksheet](https://worksheets.codalab.org/worksheets/0x7a8721f11e61436e93ac8f76da83f0e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-PQQoUv5EYI"
      },
      "source": [
        "[Leaderboard](https://https-deeplearning-ai.github.io/data-centric-comp/?utm_source=thebatch&utm_medium=newsletter&utm_campaign=dc-ai-competition&utm_content=dl-ai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1MosTohoMOU"
      },
      "source": [
        "[Data inspection discoveries](https://drive.google.com/file/d/162_c0DDd9UdTN206I9Q7A6TmJg3Fay96/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIEm0b8gkoWk"
      },
      "source": [
        "**Results**\n",
        "* 1. *Baseline dataset*\n",
        "        * Final acc: 0.67, Test acc: 0.52\n",
        "        * Final loss: 0.94, Test loss: 1.38\n",
        "\n",
        "* 2. *Data v2*\n",
        "        * Final acc: 0.78, Test acc: 0.69\n",
        "        * Final loss: 0.93, Test loss: 1.66"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiZhb6Le4i6Z"
      },
      "source": [
        "**To-Dos**\n",
        "\n",
        "**Aim:** 0.95+ accuracy \n",
        "\n",
        "✅Train the resnet baseline model on their 3k plus data towards obtaining the 0.65 accuracy score. This might require include using their 32 by 32 pixel script for image resizing, deleting corrupted files, and fixing labels.\n",
        "\n",
        "✅Inspect the 3k+ images for bad labels, bad image, etc. Fix all anomalies found. \n",
        "\n",
        "✅Download and read papers that uses the Roman MNIST dataset. We can get links to the dataset and other useful information.  \n",
        "\n",
        "\n",
        "✅Inspect the image classes distribution and take necessary actions. It's important that the dataset isn't skewed. \n",
        "\n",
        "✅Use fastai's image data augmentation functions \n",
        "\n",
        "✅Gather more Roman MNIST data in an increase rate of not more than 10x of the 3k data size. This data gathering can be done using any means available, web scraping inclusive.\n",
        "\n",
        "**N.B:** At the end of each steps above, we retrain the baseline model on the dataset in order to observe the effect on the model accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj3RYY0Rqxqu"
      },
      "source": [
        "# Notes\n",
        "\n",
        "* RENAME EACH FILES. Add an auto-increment serial number at the start of each names. This would aid easy lookup of images. Doing this will require creating a doc that contains the previous and new name.\n",
        "* the noise on some images can be erased using photo editing tools. This tool can also be used for data augmentation such as crop, resize.\n",
        "* the images labelled as 'trash' in [Data inspection discoveries](https://drive.google.com/file/d/162_c0DDd9UdTN206I9Q7A6TmJg3Fay96/view?usp=sharing) should be moved to a new folder having a new class named 'unrecognized'\n",
        "\n",
        "* in the I folder, there are images that look like small figure 1\n",
        "* some images needs to be cropped\n",
        "* some images have 1/2 vertical noisy lines\n",
        "* in the II folder, some images written with marker are so small, hence, they appear like roman numeral I.\n",
        "* in the II folder, some images have a bottom vertical noisy line\n",
        "* images' stroke are either thick or thin\n",
        "* some images are small lettered roman numeral \n",
        "* some images are capital lettered roman numeral\n",
        "* some images are capita+bars lettered roman numeral\n",
        "* some images are dashes\n",
        "* images are black on white background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBavItuW3fTg",
        "outputId": "c12aae40-4cf5-49a6-ce68-148de9215953"
      },
      "source": [
        "!git clone https://github.com/KutuDev/DataCentricAIComp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DataCentricAIComp'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 45 (delta 21), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNs1zvkp3qbv"
      },
      "source": [
        "def unzipper(in_folderPath, out_folderPath):\n",
        "    import tarfile\n",
        "\n",
        "    file = tarfile.open(in_folderPath)\n",
        "    file.extractall(out_folderPath)\n",
        "    file.close()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQm38TFf5A0o"
      },
      "source": [
        "dataset_name = 'data_v2' \n",
        "\n",
        "unzipper(in_folderPath=f'/content/DataCentricAIComp/{dataset_name}.tar.gz', out_folderPath='./images')\n",
        "unzipper(in_folderPath='/content/DataCentricAIComp/label_book.tar.gz', out_folderPath='./images')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51x_efjdj8aS"
      },
      "source": [
        "# import shutil\n",
        "\n",
        "# shutil.move(f\"/content/images/Users/Samuel/Desktop/{dataset_name}\", \"/content/images\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTnruT3c7UQk"
      },
      "source": [
        "# !pip install pysimplegui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml8h2fNe93my"
      },
      "source": [
        "# !apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "# import os\n",
        "# os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "# os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3DbbEMB7ZA2"
      },
      "source": [
        "# !python '/content/DataCentricAIComp/annotgui.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfDIr8zpROj-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "\n",
        "directory = \"/content/images\" \n",
        "user_data = directory + f\"/{dataset_name}\"\n",
        "valid_data = directory + f\"/{dataset_name}\"\n",
        "test_data = directory + \"/label_book\" # this can be the label book, or any other test set you create\n",
        "\n",
        "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\n",
        "batch_size = 8\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        user_data + '/train',\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        user_data + '/val',\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\n",
        "    if total_length > 10_000:\n",
        "        print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\n",
        "        sys.exit()\n",
        "\n",
        "    test = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        test_data,\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=False,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        input_shape=(32, 32, 3),\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "    )\n",
        "    base_model = tf.keras.Model(\n",
        "        base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n",
        "    )\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "    x = base_model(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(10)(x)\n",
        "    model = tf.keras.Model(inputs, x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    model.summary()\n",
        "    loss_0, acc_0 = model.evaluate(valid)\n",
        "    print(f\"loss {loss_0}, acc {acc_0}\")\n",
        "\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model\",\n",
        "        monitor=\"val_accuracy\",\n",
        "        mode=\"max\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train,\n",
        "        validation_data=valid,\n",
        "        epochs=100,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(\"best_model\")\n",
        "\n",
        "    loss, acc = model.evaluate(valid)\n",
        "    print(f\"final loss {loss}, final acc {acc}\")\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test)\n",
        "    print(f\"test loss {test_loss}, test acc {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKoyvFvqoT8_"
      },
      "source": [
        "# import os\n",
        "# import tarfile\n",
        "# def tardir(path, tar_name):\n",
        "#     os.chdir(r'C:\\Users\\Samuel\\Desktop')\n",
        "#     with tarfile.open(tar_name, \"w:gz\") as tar_handle:\n",
        "#         for root, dirs, files in os.walk(path):\n",
        "#             for file in files:\n",
        "#                 tar_handle.add(os.path.join(root, file))\n",
        "# tardir('data_v2', 'data_v2.tar.gz')\n",
        "# tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOJnFtmiTfwG",
        "outputId": "e3ef1890-6ffc-4760-cfb1-41de9ce77d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = model.predict(valid)\n",
        "np.argmax(y_pred[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVcO97B6alJl",
        "outputId": "34141acc-19a6-4a77-cac0-a3d36784e4f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# y_pred_labels = [np.argmax(i) for i in y_pred]\n",
        "print(y_pred_labels)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 8, 9, 6, 0, 6, 7, 2, 9, 8, 7, 6, 2, 6, 9, 6, 2, 0, 7, 4, 8, 3, 2, 7, 2, 4, 7, 2, 7, 4, 4, 9, 7, 0, 8, 2, 4, 7, 2, 5, 9, 7, 2, 2, 8, 1, 1, 5, 2, 6, 5, 3, 3, 6, 2, 9, 4, 1, 2, 6, 1, 1, 0, 4, 3, 0, 7, 4, 1, 3, 8, 8, 9, 1, 6, 8, 9, 7, 9, 1, 4, 4, 0, 7, 7, 6, 0, 9, 7, 6, 9, 9, 1, 0, 5, 8, 7, 7, 8, 2, 2, 6, 7, 9, 0, 3, 6, 3, 3, 5, 2, 0, 8, 2, 0, 6, 8, 2, 4, 6, 8, 4, 9, 6, 1, 6, 4, 5, 2, 5, 6, 5, 2, 7, 0, 3, 7, 9, 2, 9, 8, 0, 3, 0, 8, 5, 3, 6, 4, 3, 7, 9, 3, 6, 6, 7, 2, 8, 6, 9, 5, 2, 8, 1, 7, 0, 0, 6, 3, 2, 9, 2, 9, 8, 2, 3, 0, 7, 4, 5, 2, 6, 2, 0, 7, 7, 4, 8, 7, 2, 9, 6, 4, 8, 6, 1, 2, 0, 0, 2, 6, 4, 2, 2, 1, 3, 7, 7, 8, 0, 5, 1, 0, 0, 2, 3, 0, 9, 6, 1, 4, 8, 4, 5, 9, 8, 5, 0, 0, 8, 6, 7, 7, 8, 1, 8, 7, 7, 9, 5, 7, 4, 7, 3, 4, 8, 4, 6, 3, 1, 2, 8, 1, 7, 6, 4, 3, 9, 0, 2, 8, 1, 8, 8, 3, 1, 6, 4, 5, 9, 7, 4, 1, 4, 7, 4, 2, 1, 3, 9, 6, 9, 6, 3, 6, 8, 1, 7, 2, 5, 0, 6, 0, 7, 4, 6, 9, 9, 2, 6, 8, 0, 3, 8, 0, 2, 9, 8, 6, 3, 8, 4, 8, 8, 6, 7, 6, 8, 7, 9, 2, 1, 8, 6, 3, 3, 5, 4, 1, 0, 4, 2, 2, 1, 1, 0, 8, 6, 7, 8, 9, 2, 0, 9, 0, 5, 8, 3, 7, 4, 8, 7, 2, 2, 5, 1, 6, 5, 8, 3, 9, 8, 7, 9, 1, 8, 9, 1, 4, 7, 6, 0, 2, 5, 8, 7, 8, 2, 3, 3, 1, 0, 8, 8, 2, 5, 5, 0, 8, 7, 7, 1, 1, 7, 5, 5, 9, 9, 8, 9, 2, 9, 4, 4, 0, 0, 6, 5, 1, 0, 8, 5, 4, 7, 2, 1, 3, 5, 6, 5, 2, 7, 0, 3, 0, 2, 9, 0, 9, 1, 8, 2, 1, 5, 6, 2, 3, 4, 5, 5, 0, 1, 5, 6, 2, 9, 2, 1, 6, 6, 7, 3, 9, 9, 7, 8, 6, 0, 6, 3, 4, 8, 5, 0, 0, 6, 4, 4, 7, 7, 6, 2, 7, 3, 9, 4, 0, 7, 2, 6, 0, 4, 5, 8, 5, 4, 5, 1, 4, 3, 8, 9, 4, 9, 0, 6, 0, 6, 5, 8, 8, 0, 5, 6, 0, 8, 4, 6, 1, 1, 4, 0, 5, 2, 4, 9, 7, 0, 0, 9, 0, 6, 0, 9, 1, 9, 6, 0, 8, 5, 5, 9, 2, 3, 8, 9, 9, 6, 6, 9, 9, 4, 1, 2, 9, 9, 2, 2, 7, 3, 2, 8, 5, 4, 9, 9, 4, 7, 7, 4, 8, 0, 7, 5, 2, 6, 8, 8, 2, 3, 5, 3, 6, 6, 5, 0, 4, 0, 8, 6, 6, 3, 5, 2, 6, 4, 9, 8, 7, 9, 7, 8, 9, 4, 8, 6, 8, 9, 7, 6, 6, 9, 3, 2, 6, 2, 4, 2, 7, 9, 4, 9, 0, 4, 4, 9, 4, 7, 0, 6, 5, 2, 6, 5, 1, 4, 5, 5, 6, 0, 8, 4, 9, 7, 0, 4, 0, 8, 9, 4, 8, 1, 6, 0, 6, 4, 6, 6, 8, 2, 0, 7, 9, 9, 4, 2, 0, 7, 0, 9, 9, 4, 8, 7, 3, 2, 9, 2, 2, 7, 5, 5, 0, 7, 4, 9, 5, 9, 8, 3, 4, 3, 5, 4, 6, 6, 6, 5, 5, 8, 3, 4, 9, 1, 1, 7, 8, 2, 5, 8, 2, 4, 9, 6, 0, 4, 0, 6, 9, 8, 8, 9, 9, 7, 0, 8, 4, 0, 9, 9, 3, 4, 5, 9, 6, 2, 3, 2, 3, 2, 6, 3, 4, 2, 3, 2, 0, 2, 5, 9, 3, 3, 6, 5, 6, 9, 5, 8, 0, 8, 4, 1, 2, 8, 8, 1, 5, 5, 9, 4, 4, 3, 4, 0, 6, 0, 8, 7, 5, 9, 9, 5, 4, 1, 1, 0, 0, 0, 7, 1, 6, 8, 6, 4, 8, 0, 7, 7, 1, 3, 4, 0, 3, 1, 1, 9, 1, 4, 2, 3, 4, 8, 6, 4, 5, 3, 1, 1, 5, 0, 6, 7, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3TOvCCsmk6S",
        "outputId": "ef3484a5-1e25-4408-d6ab-2bcb5c127652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dir(valid)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_GeneratorState',\n",
              " '__abstractmethods__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_add_variable_with_custom_getter',\n",
              " '_apply_options',\n",
              " '_as_serialized_graph',\n",
              " '_batch_size',\n",
              " '_checkpoint_dependencies',\n",
              " '_consumers',\n",
              " '_deferred_dependencies',\n",
              " '_drop_remainder',\n",
              " '_flat_shapes',\n",
              " '_flat_structure',\n",
              " '_flat_types',\n",
              " '_functions',\n",
              " '_gather_saveables_for_checkpoint',\n",
              " '_graph',\n",
              " '_graph_attr',\n",
              " '_handle_deferred_dependencies',\n",
              " '_has_captured_ref',\n",
              " '_input_dataset',\n",
              " '_inputs',\n",
              " '_list_extra_dependencies_for_serialization',\n",
              " '_list_functions_for_serialization',\n",
              " '_lookup_dependency',\n",
              " '_map_resources',\n",
              " '_maybe_initialize_trackable',\n",
              " '_name_based_attribute_restore',\n",
              " '_name_based_restores',\n",
              " '_no_dependency',\n",
              " '_object_identifier',\n",
              " '_options',\n",
              " '_options_attr',\n",
              " '_options_tensor_to_options',\n",
              " '_preload_simple_restoration',\n",
              " '_restore_from_checkpoint_position',\n",
              " '_self_name_based_restores',\n",
              " '_self_saveable_object_factories',\n",
              " '_self_setattr_tracking',\n",
              " '_self_unconditional_checkpoint_dependencies',\n",
              " '_self_unconditional_deferred_dependencies',\n",
              " '_self_unconditional_dependency_names',\n",
              " '_self_update_uid',\n",
              " '_setattr_tracking',\n",
              " '_shape_invariant_to_type_spec',\n",
              " '_single_restoration_from_checkpoint_position',\n",
              " '_structure',\n",
              " '_tf_api_names',\n",
              " '_tf_api_names_v1',\n",
              " '_trace_variant_creation',\n",
              " '_track_trackable',\n",
              " '_tracking_metadata',\n",
              " '_type_spec',\n",
              " '_unconditional_checkpoint_dependencies',\n",
              " '_unconditional_dependency_names',\n",
              " '_update_uid',\n",
              " '_variant_tensor',\n",
              " '_variant_tensor_attr',\n",
              " '_variant_tracker',\n",
              " 'apply',\n",
              " 'as_numpy_iterator',\n",
              " 'batch',\n",
              " 'cache',\n",
              " 'cardinality',\n",
              " 'class_names',\n",
              " 'concatenate',\n",
              " 'element_spec',\n",
              " 'enumerate',\n",
              " 'file_paths',\n",
              " 'filter',\n",
              " 'flat_map',\n",
              " 'from_generator',\n",
              " 'from_tensor_slices',\n",
              " 'from_tensors',\n",
              " 'interleave',\n",
              " 'list_files',\n",
              " 'map',\n",
              " 'options',\n",
              " 'padded_batch',\n",
              " 'prefetch',\n",
              " 'range',\n",
              " 'reduce',\n",
              " 'repeat',\n",
              " 'shard',\n",
              " 'shuffle',\n",
              " 'skip',\n",
              " 'take',\n",
              " 'unbatch',\n",
              " 'window',\n",
              " 'with_options',\n",
              " 'zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad-mn1s4hvq4"
      },
      "source": [
        "# a = [i for i in valid.enumerate().as_numpy_iterator()]\n",
        "# b = np.asarray(a).astype(np.float32)\n",
        "# c = tf.convert_to_tensor(b)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxb6KKHd5D7"
      },
      "source": [
        "# tf.math.confusion_matrix(labels=b,predictions=y_pred_labels)\n",
        "# dir(valid)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDURuZyiopKg"
      },
      "source": [
        "# valid.take(0)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDSupdFDooxN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVreaB03MdsM"
      },
      "source": [
        "# Predict the values from the validation dataset\n",
        "y_pred = model.predict(valid)\n",
        "# Convert predictions classes to one hot vectors \n",
        "y_pred_classes = np.argmax(y_pred, axis = 1) \n",
        "\n",
        "y_test = \n",
        "\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Errors are difference between predicted labels and true labels\n",
        "errors = (y_pred_classes - y_test != 0)\n",
        "\n",
        "y_pred_classes_errors = y_pred_classes[errors]\n",
        "y_pred_errors = y_pred[errors]\n",
        "y_true_errors = y_test[errors]\n",
        "x_test_errors = x_test[errors]\n",
        "\n",
        "# Probabilities of the wrong predicted numbers\n",
        "y_pred_errors_prob = np.max(y_pred_errors, axis = 1)\n",
        "\n",
        "# Predicted probabilities of the true values in the error set\n",
        "true_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n",
        "\n",
        "# Difference between the probability of the predicted label and the true label\n",
        "delta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n",
        "\n",
        "# Sorted list of the delta prob errors\n",
        "sorted_dela_errors = np.argsort(delta_pred_true_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6AilxWpMfqN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 3\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((32,32,3)))\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
        "            n += 1\n",
        "\n",
        "# Show the top 6 errors\n",
        "most_important_errors = sorted_dela_errors[-6:]\n",
        "display_errors(most_important_errors, x_test_errors, y_pred_classes_errors, y_true_errors)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}